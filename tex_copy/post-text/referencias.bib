 @article{Steen_1969, title={Gaussian quadratures for the integrals $\sb{0}\sp{\infty }\,{\rm exp}(-x\sp{2})f(x)dx$ and $\sb{0}\sp{b}\,{\rm exp}(-x\sp{2})f(x)dx$}, volume={23}, ISSN={0025-5718}, url={http://dx.doi.org/10.1090/S0025-5718-1969-0247744-3}, DOI={10.1090/s0025-5718-1969-0247744-3}, number={107}, journal={Mathematics of Computation}, publisher={American Mathematical Society (AMS)}, author={Steen, N. M. and Byrne, G. D. and Gelbard, E. M.}, year={1969}, month={Sep}, pages={661–661}}

@article{Pinheiro_1996,
	title={Unconstrained parametrizations for variance-covariance matrices},
	author={José C. Pinheiro and Douglas M. Bates},
	journal={Statistics and Computing},
	year={1996},
	volume={6},
	pages={289-296}
}
@article{Liu_1994,
	ISSN = {00063444},
	URL = {http://www.jstor.org/stable/2337136},
	abstract = {For Gauss-Hermite quadrature, we consider a systematic method for transforming the variable of integration so that the integrand is sampled in an appropriate region. The effectiveness of the quadrature then depends on the ratio of the integrand to some Gaussian density being a smooth function, well approximated by a low-order polynomial. It is pointed out that, in this approach, order one Gauss-Hermite quadrature becomes the Laplace approximation. Thus the quadrature as implemented here can be thought of as a higher-order Laplace approximation.},
	author = {Qing Liu and Donald A. Pierce},
	journal = {Biometrika},
	number = {3},
	pages = {624--629},
	publisher = {[Oxford University Press, Biometrika Trust]},
	title = {A Note on Gauss-Hermite Quadrature},
	volume = {81},
	year = {1994}
}


@book{jaynes03,
	added-at = {2011-05-09T23:10:52.000+0200},
	address = {Cambridge},
	author = {Jaynes, E. T.},
	biburl = {https://www.bibsonomy.org/bibtex/2ed3616cca9af65830fb13b9f53e0f19b/josephausterwei},
	interhash = {27c58f26b65cfde811cbc41b7fe319cd},
	intrahash = {ed3616cca9af65830fb13b9f53e0f19b},
	keywords = {imported},
	publisher = {Cambridge University Press},
	timestamp = {2011-05-10T10:42:42.000+0200},
	title = {Probability theory: The logic of science},
	year = 2003
}

@book{Robert_2005,
	author = {Robert, Christian P. and Casella, George},
	title = {Monte Carlo Statistical Methods (Springer Texts in Statistics)},
	year = {2005},
	isbn = {0387212396},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
} 

@article{Ghahramani_2013, title={Bayesian non-parametrics and the probabilistic approach to modelling}, volume={371}, ISSN={1471-2962}, url={http://dx.doi.org/10.1098/rsta.2011.0553}, DOI={10.1098/rsta.2011.0553}, number={1984}, journal={Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences}, publisher={The Royal Society}, author={Ghahramani, Zoubin}, year={2013}, month={Feb}, pages={20110553}}

@book{Hjort_2010,
	added-at = {2011-05-09T23:10:52.000+0200},
	address = {Cambridge, UK},
	author = {Hjort, N. and Holmes, C. and Mueller, P. and Walker, S.},
	biburl = {https://www.bibsonomy.org/bibtex/259f342a4a97fb061a56fd23cdb5ad9a5/josephausterwei},
	interhash = {ae9c58f4321fea1247b1843b50c07082},
	intrahash = {59f342a4a97fb061a56fd23cdb5ad9a5},
	keywords = {imported},
	publisher = {Cambridge University Press},
	timestamp = {2011-05-10T10:42:42.000+0200},
	title = {Bayesian Nonparametrics: Principles and Practice},
	year = 2010
}


@book{Brooks_2011,
	added-at = {2015-03-24T17:28:34.000+0100},
	author = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
	biburl = {https://www.bibsonomy.org/bibtex/22b8d02bec832fa945b62ecf7808614bf/becker},
	interhash = {0b127e40d41a970274484b65a7e0744f},
	intrahash = {2b8d02bec832fa945b62ecf7808614bf},
	keywords = {carlo chain diss handbook inthesis markov mcmc monte},
	publisher = {CRC press},
	timestamp = {2017-08-04T09:03:42.000+0200},
	title = {Handbook of Markov Chain Monte Carlo},
	year = 2011
}

@ARTICLE{MacKay_1991,
	author = {David J.C. MacKay},
	title = {Bayesian Interpolation},
	journal = {NEURAL COMPUTATION},
	year = {1991},
	volume = {4},
	pages = {415--447}
}

@incollection{Rasmussen_2001,
	title = {Occam's Razor},
	author = {Carl Edward Rasmussen and Ghahramani, Zoubin},
	booktitle = {Advances in Neural Information Processing Systems 13},
	editor = {T. K. Leen and T. G. Dietterich and V. Tresp},
	pages = {294--300},
	year = {2001},
	publisher = {MIT Press},
	url = {http://papers.nips.cc/paper/1925-occams-razor.pdf}
}

@ARTICLE{Betancourt_2017,
	author = {{Betancourt}, Michael},
	title = "{A Conceptual Introduction to Hamiltonian Monte Carlo}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Methodology},
	year = "2017",
	month = "Jan",
	eid = {arXiv:1701.02434},
	pages = {arXiv:1701.02434},
	archivePrefix = {arXiv},
	eprint = {1701.02434},
	primaryClass = {stat.ME},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170102434B},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@book{MacKay2003,
	added-at = {2007-05-24T14:43:04.000+0200},
	author = {MacKay, David J. C.},
	biburl = {https://www.bibsonomy.org/bibtex/24c23fea472f6e75c0964badd83883d77/tmalsburg},
	interhash = {86f621d9d6f9f159448f768d792d4511},
	intrahash = {4c23fea472f6e75c0964badd83883d77},
	keywords = {bayesianinference book informationtheory neuralnetworks patternrecognition probabilitytheory},
	publisher = {Copyright Cambridge University Press},
	timestamp = {2007-05-24T14:43:04.000+0200},
	title = {Information Theory, Inference, and Learning Algorithms},
	year = 2003
}

@article{Cox_1963, title={The Algebra of Probable Inference}, volume={31}, ISSN={1943-2909}, url={http://dx.doi.org/10.1119/1.1969248}, DOI={10.1119/1.1969248}, number={1}, journal={American Journal of Physics}, publisher={American Association of Physics Teachers (AAPT)}, author={Cox, Richard T. and Jaynes, E. T.}, year={1963}, month={Jan}, pages={66–67}}

 @article{Shawe_Taylor_2004, title={Kernel Methods for Pattern Analysis}, ISBN={9780511809682}, url={http://dx.doi.org/10.1017/CBO9780511809682}, DOI={10.1017/cbo9780511809682}, publisher={Cambridge University Press}, author={Shawe-Taylor, John and Cristianini, Nello}, year={2004}}

@INPROCEEDINGS{Rasmussen06,
	author = {Carl Edward Rasmussen},
	title = {Gaussian processes for machine learning},
	booktitle = {},
	year = {2006},
	publisher = {MIT Press}
}

@article{Dudley_2002, title={Real Analysis and Probability}, ISBN={9780511755347}, url={http://dx.doi.org/10.1017/CBO9780511755347}, DOI={10.1017/cbo9780511755347}, publisher={Cambridge University Press}, author={Dudley, R. M.}, year={2002}}

@ARTICLE{Kanagawa_2018,
	author = {{Kanagawa}, Motonobu and {Hennig}, Philipp and {Sejdinovic}, Dino and
	{Sriperumbudur}, Bharath K},
	title = "{Gaussian Processes and Kernel Methods: A Review on Connections and Equivalences}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	year = "2018",
	month = "Jul",
	eid = {arXiv:1807.02582},
	pages = {arXiv:1807.02582},
	archivePrefix = {arXiv},
	eprint = {1807.02582},
	primaryClass = {stat.ML},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180702582K},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@book{hogben14,
	abstract = {"Preface to the Second Edition Both the format and guiding vision of Handbook of Linear Algebra remain unchanged, but a substantial amount of new material has been included in the second edition. The length has increased from 1400 pages to 1900 pages. There are 20 new chapters. Subjects such as Schur complements, special types of matrices, generalized inverses, matrices over nite elds, and invariant subspaces are now treated in separate chapters. There are additional chapters on applications of linear algebra, for example, to epidemiology. There is a new chapter on using the free open source computer mathematics system Sage for linear algebra, which also provides a general introduction to Sage. Additional surveys of currently active research topics such as tournaments are also included. Many of the existing articles have been revised and updated, in some cases adding a substantial amount of new material. For example, the chapters on sign pattern matrices and on applications to geometry have additional sections. As was true in the rst edition, the topics range from the most basic linear algebra to advanced topics including background for active research areas. In this edition, many of the chapters on advanced topics now include Conjectures and Open Problems, either as a part of some sections or as a new section at the end of the chapter. The conjectures and questions listed in such sections have been in the literature for more than ve years at the time of writing, and often a number of partial results have been obtained. In most cases, the current (at the time of writing) state of research related to the question is summarized as facts. Of course, there is no guarantee that (years after the writing date) such problems have not been solved (in fact, we hope they ha"--},
	added-at = {2014-07-07T01:42:04.000+0200},
	author = {Hogben, Leslie},
	biburl = {https://www.bibsonomy.org/bibtex/24fb697e4c1dee84cc03f687619a70179/ytyoun},
	edition = {2nd},
	interhash = {f36856688b40f7236dd6fcbbe4f4d486},
	intrahash = {4fb697e4c1dee84cc03f687619a70179},
	isbn = {9781466507289 1466507284},
	keywords = {characteristic graph.theory handbook hogben linear.algebra matrix polynomial textbook},
	refid = {858081452},
	timestamp = {2016-12-28T03:25:48.000+0100},
	title = {Handbook of Linear Algebra},
	year = 2014
}

 @article{Stein_1999, title={Interpolation of Spatial Data}, ISBN={9781461214946}, ISSN={0172-7397}, url={http://dx.doi.org/10.1007/978-1-4612-1494-6}, DOI={10.1007/978-1-4612-1494-6}, journal={Springer Series in Statistics}, publisher={Springer New York}, author={Stein, Michael L.}, year={1999}}
 
 @book{Chatfield_2004,
 	added-at = {2008-12-09T09:45:02.000+0100},
 	address = {Florida, US},
 	author = {Chatfield, Chris},
 	biburl = {https://www.bibsonomy.org/bibtex/213565af2e063e6e81ffb11f95c6e84e0/ans},
 	edition = {6th},
 	interhash = {64e539559e9e18f5155d8bd0338a05a1},
 	intrahash = {13565af2e063e6e81ffb11f95c6e84e0},
 	keywords = {bachelor introduction series time ws08},
 	publisher = {CRC Press},
 	timestamp = {2011-03-22T23:02:16.000+0100},
 	title = {The analysis of time series: an introduction},
 	year = 2004
 }
 
@book{CIS-4647,
	Author = {Adler, Robert J.},
	Publisher = {John Wiley and Sons},
	Title = {The geometry of random fields},
	Year = 1981
}

@inproceedings{wilson2013gaussian,
	added-at = {2019-05-13T04:22:35.000+0200},
	author = {Wilson, Andrew and Adams, Ryan},
	biburl = {https://www.bibsonomy.org/bibtex/232575f48e98643bcf27a2cb505cff524/becker},
	booktitle = {International Conference on Machine Learning},
	interhash = {3f9c7df7eb626bbe62a04ebf1b501960},
	intrahash = {32575f48e98643bcf27a2cb505cff524},
	keywords = {bayesian discovery gaussian gp model pattern process},
	pages = {1067--1075},
	timestamp = {2019-05-13T04:22:47.000+0200},
	title = {Gaussian process kernels for pattern discovery and extrapolation},
	year = 2013
}

@ARTICLE{Haitao_2018,
	author = {{Liu}, Haitao and {Ong}, Yew-Soon and {Shen}, Xiaobo and {Cai}, Jianfei},
	title = "{When Gaussian Process Meets Big Data: A Review of Scalable GPs}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	year = "2018",
	month = "Jul",
	eid = {arXiv:1807.01065},
	pages = {arXiv:1807.01065},
	archivePrefix = {arXiv},
	eprint = {1807.01065},
	primaryClass = {stat.ML},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180701065L},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Eckart_1936, title={The approximation of one matrix by another of lower rank}, volume={1}, ISSN={1860-0980}, url={http://dx.doi.org/10.1007/BF02288367}, DOI={10.1007/bf02288367}, number={3}, journal={Psychometrika}, publisher={Springer Science and Business Media LLC}, author={Eckart, Carl and Young, Gale}, year={1936}, month={Sep}, pages={211–218}}

@INPROCEEDINGS{Williams01usingthe,
	author = {Christopher Williams and Matthias Seeger},
	title = {Using the Nyström Method to Speed Up Kernel Machines},
	booktitle = {Advances in Neural Information Processing Systems 13},
	year = {2001},
	pages = {682--688},
	publisher = {MIT Press}
}

@article{Fowlkes_2004, title={Spectral grouping using the nystrom method}, volume={26}, ISSN={0162-8828}, url={http://dx.doi.org/10.1109/TPAMI.2004.1262185}, DOI={10.1109/tpami.2004.1262185}, number={2}, journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, publisher={Institute of Electrical and Electronics Engineers (IEEE)}, author={Fowlkes, C. and Belongie, S. and Fan Chung and Malik, J.}, year={2004}, month={Feb}, pages={214–225}}

@article{Wang_2009, title={Kernel Nyström method for light transport}, volume={28}, ISSN={0730-0301}, url={http://dx.doi.org/10.1145/1531326.1531335}, DOI={10.1145/1531326.1531335}, number={3}, journal={ACM Transactions on Graphics}, publisher={Association for Computing Machinery (ACM)}, author={Wang, Jiaping and Dong, Yue and Tong, Xin and Lin, Zhouchen and Guo, Baining}, year={2009}, month={Jul}, pages={1}}

@ARTICLE{Gittens_2011,
	author = {{Gittens}, Alex},
	title = "{The spectral norm error of the naive Nystrom extension}",
	journal = {arXiv e-prints},
	keywords = {Mathematics - Numerical Analysis, Computer Science - Numerical Analysis},
	year = "2011",
	month = "Oct",
	eid = {arXiv:1110.5305},
	pages = {arXiv:1110.5305},
	archivePrefix = {arXiv},
	eprint = {1110.5305},
	primaryClass = {math.NA},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2011arXiv1110.5305G},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Candela_2005,
	title = {A Unifying View of Sparse Approximate Gaussian Process Regression},
	author = {Quinonero Candela, J. and Rasmussen, CE.},
	journal = {Journal of Machine Learning Research},
	volume = {6},
	pages = {1935-1959},
	organization = {Max-Planck-Gesellschaft},
	school = {Biologische Kybernetik},
	month = dec,
	year = {2005},
	month_numeric = {12}
}

@InProceedings{Hennig_2012,
	author = 	 {Hennig, P. and Kiefel, M.},
	title = 	 {Quasi-Newton methods -- a new direction},
	booktitle =    {Int. Conf. on Machine Learning (ICML)},
	year = 	 {2012},
	volume = 	 {29}
}

@INPROCEEDINGS{Smola_2001,
	author = {Alex J. Smola and Peter Bartlett},
	title = {Sparse Greedy Gaussian Process Regression},
	booktitle = {Advances in Neural Information Processing Systems 13},
	year = {2001},
	pages = {619--625},
	publisher = {MIT Press}
}

@INPROCEEDINGS{Seeger_2003,
	author = {Matthias Seeger and Christopher K. I. Williams and Neil D. Lawrence},
	title = {Fast Forward Selection to Speed Up Sparse Gaussian Process Regression},
	booktitle = {IN WORKSHOP ON AI AND STATISTICS 9},
	year = {2003},
	publisher = {}
}

@INPROCEEDINGS{Snelson_2006,
	author = {Edward Snelson and Zoubin Ghahramani},
	title = {Sparse Gaussian processes using pseudo-inputs},
	booktitle = {Advances in Neural Information Processing Systems 18},
	year = {2006},
	pages = {1257--1264},
	publisher = {MIT press}
}

@article{Silverman_1985, title={Some Aspects of the Spline Smoothing Approach to Non-Parametric Regression Curve Fitting}, volume={47}, ISSN={0035-9246}, url={http://dx.doi.org/10.1111/j.2517-6161.1985.tb01327.x}, DOI={10.1111/j.2517-6161.1985.tb01327.x}, number={1}, journal={Journal of the Royal Statistical Society: Series B (Methodological)}, publisher={Wiley}, author={Silverman, B. W.}, year={1985}, month={Sep}, pages={1–21}}

@INPROCEEDINGS{Titsias_2009,
	author = {Michalis K. Titsias},
	title = {Variational learning of inducing variables in sparse Gaussian processes},
	booktitle = {In Artificial Intelligence and Statistics 12},
	year = {2009},
	pages = {567--574}
}

@ARTICLE{Bui_2016,
	author = {{Bui}, Thang D. and {Yan}, Josiah and {Turner}, Richard E.},
	title = "{A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	year = "2016",
	month = "May",
	eid = {arXiv:1605.07066},
	pages = {arXiv:1605.07066},
	archivePrefix = {arXiv},
	eprint = {1605.07066},
	primaryClass = {stat.ML},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160507066B},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{O_Hagan_1991, title={Bayes–Hermite quadrature}, volume={29}, ISSN={0378-3758}, url={http://dx.doi.org/10.1016/0378-3758(91)90002-V}, DOI={10.1016/0378-3758(91)90002-v}, number={3}, journal={Journal of Statistical Planning and Inference}, publisher={Elsevier BV}, author={O’Hagan, A.}, year={1991}, month={Nov}, pages={245–260}}

@incollection{Ghahramani_2003,
	title = {Bayesian Monte Carlo},
	author = {Ghahramani, Zoubin and Carl E. Rasmussen},
	booktitle = {Advances in Neural Information Processing Systems 15},
	editor = {S. Becker and S. Thrun and K. Obermayer},
	pages = {505--512},
	year = {2003},
	publisher = {MIT Press},
	url = {http://papers.nips.cc/paper/2150-bayesian-monte-carlo.pdf}
}

@article{Epanechnikov_1969, title={Non-Parametric Estimation of a Multivariate Probability Density}, volume={14}, ISSN={1095-7219}, url={http://dx.doi.org/10.1137/1114019}, DOI={10.1137/1114019}, number={1}, journal={Theory of Probability and Its Applications}, publisher={Society for Industrial & Applied Mathematics (SIAM)}, author={Epanechnikov, V. A.}, year={1969}, month={Jan}, pages={153–158}}

@incollection{Osborne_2012,
	title = {Active Learning of Model Evidence Using Bayesian Quadrature},
	author = {Michael Osborne and Garnett, Roman and Ghahramani, Zoubin and Duvenaud, David K and Roberts, Stephen J and Carl E. Rasmussen},
	booktitle = {Advances in Neural Information Processing Systems 25},
	editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
	pages = {46--54},
	year = {2012},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/4657-active-learning-of-model-evidence-using-bayesian-quadrature.pdf}
}

@inproceedings{Gunter_2014,
	title = {Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature},
	author = {Gunter, T. and Osborne, M. and Garnett, R. and Hennig, P. and Roberts, S.},
	booktitle = {Advances in Neural Information Processing Systems 27},
	pages = {2789--2797},
	publisher = {Curran Associates, Inc.},
	year = {2014},
	url = {http://papers.nips.cc/paper/5483-sampling-for-inference-in-probabilistic-models-with-fast-bayesian-quadrature.pdf}
}

@InProceedings{Chai_2019,
	title = 	 {Improving Quadrature for Constrained Integrands},
	author = 	 {Chai, Henry R. and Garnett, Roman},
	booktitle = 	 {Proceedings of Machine Learning Research},
	pages = 	 {2751--2759},
	year = 	 {2019},
	volume = 	 {89},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {},
	month = 	 {16--18 Apr},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v89/chai19a/chai19a.pdf},
	url = 	 {http://proceedings.mlr.press/v89/chai19a.html},
	abstract = 	 {We present an improved Bayesian framework for performing inference of affine transformations of constrained functions. We focus on quadrature with nonnegative functions, a common task in Bayesian inference.  We consider constraints on the range of the function of interest, such as nonnegativity or boundedness. Although our framework is general, we derive explicit approximation schemes for these constraints, and argue for the use of a log transformation for functions with high dynamic range such as likelihood surfaces. We propose a novel method for optimizing hyperparameters in this framework: we optimize the marginal likelihood in the original space, as opposed to in the transformed space. The result is a model that better explains the actual data. Experiments on synthetic and real-world data demonstrate our framework achieves superior estimates using less wall-clock time than existing Bayesian quadrature procedures.}
}

@inproceedings{Briol_2015,
	author = {Briol, Fran\c{c}ois-Xavier and Oates, Chris J. and Girolami, Mark and Osborne, Michael A.},
	title = {Frank-Wolfe Bayesian Quadrature: Probabilistic Integration with Theoretical Guarantees},
	booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
	series = {NIPS'15},
	year = {2015},
	location = {Montreal, Canada},
	pages = {1162--1170},
	numpages = {9},
	url = {http://dl.acm.org/citation.cfm?id=2969239.2969369},
	acmid = {2969369},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
} 

@ARTICLE{Yingzhen_2016,
	author = {{Li}, Yingzhen and {Turner}, Richard E.},
	title = "{R\textbackslash'enyi Divergence Variational Inference}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	year = "2016",
	month = "Feb",
	eid = {arXiv:1602.02311},
	pages = {arXiv:1602.02311},
	archivePrefix = {arXiv},
	eprint = {1602.02311},
	primaryClass = {stat.ML},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160202311L},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Hernandes-Lobato_2015,
	author = {{Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel and {Li}, Yingzhen and {Rowland
	}, Mark and {Hern{\'a}ndez-Lobato}, Daniel and {Bui}, Thang and
	{Turner}, Richard E.},
	title = "{Black-box $\alpha$-divergence Minimization}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Machine Learning},
	year = "2015",
	month = "Nov",
	eid = {arXiv:1511.03243},
	pages = {arXiv:1511.03243},
	archivePrefix = {arXiv},
	eprint = {1511.03243},
	primaryClass = {stat.ML},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2015arXiv151103243H},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@incollection{Wang_2018,
	title = {Variational Inference with Tail-adaptive f-Divergence},
	author = {Wang, Dilin and Liu, Hao and Liu, Qiang},
	booktitle = {Advances in Neural Information Processing Systems 31},
	pages = {5737--5747},
	year = {2018},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/7816-variational-inference-with-tail-adaptive-f-divergence.pdf}
}

@article{Blei_2017, title={Variational Inference: A Review for Statisticians}, volume={112}, ISSN={1537-274X}, url={http://dx.doi.org/10.1080/01621459.2017.1285773}, DOI={10.1080/01621459.2017.1285773}, number={518}, journal={Journal of the American Statistical Association}, publisher={Informa UK Limited}, author={Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.}, year={2017}, month={Feb}, pages={859–877}}

@article{Zhang_2019, title={Advances in Variational Inference}, volume={41}, ISSN={1939-3539}, url={http://dx.doi.org/10.1109/TPAMI.2018.2889774}, DOI={10.1109/tpami.2018.2889774}, number={8}, journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, publisher={Institute of Electrical and Electronics Engineers (IEEE)}, author={Zhang, Cheng and Butepage, Judith and Kjellstrom, Hedvig and Mandt, Stephan}, year={2019}, month={Aug}, pages={2008–2026}}

@book{Bishop_2007,
	added-at = {2009-06-02T09:46:22.000+0200},
	asin = {0387310738},
	author = {Bishop, Christopher M.},
	biburl = {https://www.bibsonomy.org/bibtex/2d21de30a3a67c0f9f3c96bd6eec3267a/midtiby},
	description = {Amazon.com: Pattern Recognition and Machine Learning (Information Science and Statistics): Christopher M. Bishop: Books},
	dewey = {006.4},
	ean = {9780387310732},
	edition = 1,
	interhash = {f6fec2ccd82dec0dcd63825e301662cf},
	intrahash = {d21de30a3a67c0f9f3c96bd6eec3267a},
	isbn = {0387310738},
	keywords = {algorithms machinelearning patternrecognition statistics},
	publisher = {Springer},
	timestamp = {2009-06-02T15:22:29.000+0200},
	title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
	url = {http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738%3FSubscriptionId%3D13CT5CVB80YFWJEPWS02%26tag%3Dws%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0387310738},
	year = 2007
}

@book{Murphy_2012,
	added-at = {2016-11-26T13:10:09.000+0100},
	author = {Murphy, K.},
	biburl = {https://www.bibsonomy.org/bibtex/250971999eaaaa64a40439c5086a2ba17/machinelearning},
	interhash = {997486e4e92adad73560717a6a07bf82},
	intrahash = {50971999eaaaa64a40439c5086a2ba17},
	keywords = {ml},
	publisher = {MIT Press},
	timestamp = {2016-11-26T13:17:02.000+0100},
	title = {Machine Learning: a Probabilistic Perspective},
	year = 2012
}

 @article{Boyd_2004, title={Convex Optimization}, ISBN={9780511804441}, url={http://dx.doi.org/10.1017/CBO9780511804441}, DOI={10.1017/cbo9780511804441}, publisher={Cambridge University Press}, author={Boyd, Stephen and Vandenberghe, Lieven}, year={2004}}

@article{Duchi_2011,
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
	journal = {J. Mach. Learn. Res.},
	issue_date = {2/1/2011},
	volume = {12},
	month = jul,
	year = {2011},
	issn = {1532-4435},
	pages = {2121--2159},
	numpages = {39},
	url = {http://dl.acm.org/citation.cfm?id=1953048.2021068},
	acmid = {2021068},
	publisher = {JMLR.org},
} 

@ARTICLE{Kingma_2014,
	author = {{Kingma}, Diederik P. and {Ba}, Jimmy},
	title = "{Adam: A Method for Stochastic Optimization}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Machine Learning},
	year = "2014",
	month = "Dec",
	eid = {arXiv:1412.6980},
	pages = {arXiv:1412.6980},
	archivePrefix = {arXiv},
	eprint = {1412.6980},
	primaryClass = {cs.LG},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.6980K},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

 @article{Qian_1999, title={On the momentum term in gradient descent learning algorithms}, volume={12}, ISSN={0893-6080}, url={http://dx.doi.org/10.1016/S0893-6080(98)00116-6}, DOI={10.1016/s0893-6080(98)00116-6}, number={1}, journal={Neural Networks}, publisher={Elsevier BV}, author={Qian, Ning}, year={1999}, month={Jan}, pages={145–151}}

@ARTICLE{Ruder_2016,
	author = {{Ruder}, Sebastian},
	title = "{An overview of gradient descent optimization algorithms}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Machine Learning},
	year = "2016",
	month = "Sep",
	eid = {arXiv:1609.04747},
	pages = {arXiv:1609.04747},
	archivePrefix = {arXiv},
	eprint = {1609.04747},
	primaryClass = {cs.LG},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160904747R},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Robbins_1951,
	added-at = {2008-10-07T16:03:39.000+0200},
	author = {Robbins, H. and Monro, S.},
	biburl = {https://www.bibsonomy.org/bibtex/2cc1b9aa8927ac4952e93f34094a3eaaf/brefeld},
	interhash = {93d54534a08c30eda9e34d1def03ffa3},
	intrahash = {cc1b9aa8927ac4952e93f34094a3eaaf},
	journal = {Annals of Mathematical Statistics},
	keywords = {imported},
	pages = {400-407},
	timestamp = {2008-10-07T16:03:40.000+0200},
	title = {A stochastic approximation method},
	volume = 22,
	year = 1951
}

@InProceedings{Ranganath_2014,
	title = 	 {{Black Box Variational Inference}},
	author = 	 {Rajesh Ranganath and Sean Gerrish and David Blei},
	booktitle = 	 {Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics},
	pages = 	 {814--822},
	year = 	 {2014},
	volume = 	 {33},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Reykjavik, Iceland},
	month = 	 {22--25 Apr},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v33/ranganath14.pdf},
	url = 	 {http://proceedings.mlr.press/v33/ranganath14.html},
	abstract = 	 {Variational inference has become a widely used method to approximate posteriors in complex latent variables models.  However, deriving a variational inference algorithm generally requires significant model-specific analysis. These efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand.  In this paper, we present a “black box” variational inference algorithm, one that can be quickly applied to many models with little additional derivation.  Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution.  We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations.  We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.}
}

@ARTICLE{Wingate_2013,
	author = {{Wingate}, David and {Weber}, Theophane},
	title = "{Automated Variational Inference in Probabilistic Programming}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	year = "2013",
	month = "Jan",
	eid = {arXiv:1301.1299},
	pages = {arXiv:1301.1299},
	archivePrefix = {arXiv},
	eprint = {1301.1299},
	primaryClass = {stat.ML},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2013arXiv1301.1299W},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Kingma_2013,
	author = {{Kingma}, Diederik P and {Welling}, Max},
	title = "{Auto-Encoding Variational Bayes}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	year = "2013",
	month = "Dec",
	eid = {arXiv:1312.6114},
	pages = {arXiv:1312.6114},
	archivePrefix = {arXiv},
	eprint = {1312.6114},
	primaryClass = {stat.ML},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2013arXiv1312.6114K},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@incollection{Titsias_2015,
	title = {Local Expectation Gradients for Black Box Variational Inference},
	author = {Titsias RC AUEB, Michalis and L\'{a}zaro-Gredilla, Miguel},
	booktitle = {Advances in Neural Information Processing Systems 28},
	pages = {2638--2646},
	year = {2015},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/5678-local-expectation-gradients-for-black-box-variational-inference.pdf}
}

@ARTICLE{Ruiz_2016,
	author = {{Ruiz}, Francisco J.~R. and {Titsias}, Michalis K. and {Blei}, David M.},
	title = "{Overdispersed Black-Box Variational Inference}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Machine Learning},
	year = "2016",
	month = "Mar",
	eid = {arXiv:1603.01140},
	pages = {arXiv:1603.01140},
	archivePrefix = {arXiv},
	eprint = {1603.01140},
	primaryClass = {stat.ML},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160301140R},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Bingham_2018,
	author = {{Bingham}, Eli and {Chen}, Jonathan P. and {Jankowiak}, Martin and
	{Obermeyer}, Fritz and {Pradhan}, Neeraj and {Karaletsos}, Theofanis and
	{Singh}, Rohit and {Szerlip}, Paul and {Horsfall}, Paul and
	{Goodman}, Noah D.},
	title = "{Pyro: Deep Universal Probabilistic Programming}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages, Statistics - Machine Learning},
	year = "2018",
	month = "Oct",
	eid = {arXiv:1810.09538},
	pages = {arXiv:1810.09538},
	archivePrefix = {arXiv},
	eprint = {1810.09538},
	primaryClass = {cs.LG},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv181009538B},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Tran_2016,
	author = {{Tran}, Dustin and {Kucukelbir}, Alp and {Dieng}, Adji B. and
	{Rudolph}, Maja and {Liang}, Dawen and {Blei}, David M.},
	title = "{Edward: A library for probabilistic modeling, inference, and criticism}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Computation, Computer Science - Artificial Intelligence, Computer Science - Programming Languages, Statistics - Applications, Statistics - Machine Learning},
	year = "2016",
	month = "Oct",
	eid = {arXiv:1610.09787},
	pages = {arXiv:1610.09787},
	archivePrefix = {arXiv},
	eprint = {1610.09787},
	primaryClass = {stat.CO},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv161009787T},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Gershman_2012,
	author = {{Gershman}, Samuel and {Hoffman}, Matt and {Blei}, David},
	title = "{Nonparametric variational inference}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	year = "2012",
	month = "Jun",
	eid = {arXiv:1206.4665},
	pages = {arXiv:1206.4665},
	archivePrefix = {arXiv},
	eprint = {1206.4665},
	primaryClass = {cs.LG},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2012arXiv1206.4665G},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@Inbook{Jaakkola_1998,
	author="Jaakkola, Tommi S.
	and Jordan, Michael I.",
	editor="Jordan, Michael I.",
	title="Improving the Mean Field Approximation Via the Use of Mixture Distributions",
	bookTitle="Learning in Graphical Models",
	year="1998",
	publisher="Springer Netherlands",
	address="Dordrecht",
	pages="163--173",
	abstract="Mean field methods provide computationally efficient approximations to posterior probability distributions for graphical models. Simple mean field methods make a completely factorized approximation to the posterior, which is unlikely to be accurate when the posterior is multimodal. Indeed, if the posterior is multi-modal, only one of the modes can be captured. To improve the mean field approximation in such cases, we employ mixture models as posterior approximations, where each mixture component is a factorized distribution. We describe efficient methods for optimizing the Parameters in these models.",
	isbn="978-94-011-5014-9",
	doi="10.1007/978-94-011-5014-9_6",
	url="https://doi.org/10.1007/978-94-011-5014-9_6"
}

@incollection{Bishop_1997,
	title = {Approximating Posterior Distributions in Belief Networks Using Mixtures},
	author = {Christopher M. Bishop and Neil D. Lawrence and Jaakkola, Tommi and Michael I. Jordan},
	booktitle = {Advances in Neural Information Processing Systems 10},
	editor = {M. I. Jordan and M. J. Kearns and S. A. Solla},
	pages = {416--422},
	year = {1998},
	publisher = {MIT Press},
	url = {http://papers.nips.cc/paper/1392-approximating-posterior-distributions-in-belief-networks-using-mixtures.pdf}
}

@article{Salimans_2012,
	author = {Salimans, Tim and Knowles, David},
	year = {2012},
	month = {06},
	pages = {},
	title = {Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression},
	volume = {8},
	journal = {Bayesian Analysis},
	doi = {10.1214/13-BA858}
}

@incollection{Locatelo_2018,
	title = {Boosting Black Box Variational Inference},
	author = {Locatello, Francesco and Dresdner, Gideon and Khanna, Rajiv and Valera, Isabel and Raetsch, Gunnar},
	booktitle = {Advances in Neural Information Processing Systems 31},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	pages = {3401--3411},
	year = {2018},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/7600-boosting-black-box-variational-inference.pdf}
}

@ARTICLE{Guo_2016,
	author = {{Guo}, Fangjian and {Wang}, Xiangyu and {Fan}, Kai and
	{Broderick}, Tamara and {Dunson}, David B.},
	title = "{Boosting Variational Inference}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	year = "2016",
	month = "Nov",
	eid = {arXiv:1611.05559},
	pages = {arXiv:1611.05559},
	archivePrefix = {arXiv},
	eprint = {1611.05559},
	primaryClass = {stat.ML},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv161105559G},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Miller_2016,
	author = {{Miller}, Andrew C. and {Foti}, Nicholas and {Adams}, Ryan P.},
	title = "{Variational Boosting: Iteratively Refining Posterior Approximations}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Statistics - Methodology},
	year = "2016",
	month = "Nov",
	eid = {arXiv:1611.06585},
	pages = {arXiv:1611.06585},
	archivePrefix = {arXiv},
	eprint = {1611.06585},
	primaryClass = {stat.ML},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv161106585M},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@InProceedings{Arenz_2018,
	title = 	 {Efficient Gradient-Free Variational Inference using Policy Search},
	author = 	 {Arenz, Oleg and Neumann, Gerhard and Zhong, Mingjun},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {234--243},
	year = 	 {2018},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Stockholmsmässan, Stockholm Sweden},
	month = 	 {10--15 Jul},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v80/arenz18a/arenz18a.pdf},
	url = 	 {http://proceedings.mlr.press/v80/arenz18a.html},
	abstract = 	 {Inference from complex distributions is a common problem in machine learning needed for many Bayesian methods. We propose an efficient, gradient-free method for learning general GMM approximations of multimodal distributions based on recent insights from stochastic search methods. Our method establishes information-geometric trust regions to ensure efficient exploration of the sampling space and stability of the GMM updates, allowing for efficient estimation of multi-variate Gaussian variational distributions. For GMMs, we apply a variational lower bound to decompose the learning objective into sub-problems given by learning the individual mixture components and the coefficients. The number of mixture components is adapted online in order to allow for arbitrary exact approximations. We demonstrate on several domains that we can learn significantly better approximations than competing variational inference methods and that the quality of samples drawn from our approximations is on par with samples created by state-of-the-art MCMC samplers that require significantly more computational resources.}
}

@InProceedings{Jankowiak_2019,
	title = 	 {Pathwise Derivatives for Multivariate Distributions},
	author = 	 {Jankowiak, Martin and Karaletsos, Theofanis},
	booktitle = 	 {Proceedings of Machine Learning Research},
	pages = 	 {333--342},
	year = 	 {2019},
	volume = 	 {89},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {},
	month = 	 {16--18 Apr},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v89/jankowiak19a/jankowiak19a.pdf},
	url = 	 {http://proceedings.mlr.press/v89/jankowiak19a.html},
	abstract = 	 {We exploit the link between the transport equation and derivatives of expectations to construct efficient pathwise gradient estimators for multivariate distributions. We focus on two main threads. First, we use null solutions of the transport equation to construct adaptive control variates that can be used to construct gradient estimators with reduced variance. Second, we consider the case of multivariate mixture distributions. In particular we show how to compute pathwise derivatives for mixtures of multivariate Normal distributions with arbitrary means and diagonal covariances. We demonstrate in a variety of experiments in the context of variational inference that our gradient estimators can outperform other methods, especially in high dimensions.}
}

@incollection{Acerbi_2018,
	title = {Variational Bayesian Monte Carlo},
	author = {Acerbi, Luigi},
	booktitle = {Advances in Neural Information Processing Systems 31},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	pages = {8213--8223},
	year = {2018},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/8043-variational-bayesian-monte-carlo.pdf}
}

@article{Friedman_2000,
	author = "Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert",
	doi = "10.1214/aos/1016218223",
	fjournal = "The Annals of Statistics",
	journal = "Ann. Statist.",
	month = "04",
	number = "2",
	pages = "337--407",
	publisher = "The Institute of Mathematical Statistics",
	title = "Additive logistic regression: a statistical view of boosting (With 		 discussion and a rejoinder by the authors)",
	url = "https://doi.org/10.1214/aos/1016218223",
	volume = "28",
	year = "2000"
}

@INPROCEEDINGS{Freund_1999,
	author = {Yoav Freund and Robert E. Schapire},
	title = {A Short Introduction to Boosting},
	booktitle = {In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence},
	year = {1999},
	pages = {1401--1406},
	publisher = {Morgan Kaufmann}
}

@article{Freund_1997,
	title = "A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting",
	journal = "Journal of Computer and System Sciences",
	volume = "55",
	number = "1",
	pages = "119 - 139",
	year = "1997",
	issn = "0022-0000",
	doi = "https://doi.org/10.1006/jcss.1997.1504",
	url = "http://www.sciencedirect.com/science/article/pii/S002200009791504X",
	author = "Yoav Freund and Robert E Schapire",
	abstract = "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone–Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line."
}

 @article{Tong_Zhang_2003, title={Sequential greedy approximation for certain convex optimization problems}, volume={49}, ISSN={0018-9448}, url={http://dx.doi.org/10.1109/TIT.2002.808136}, DOI={10.1109/tit.2002.808136}, number={3}, journal={IEEE Transactions on Information Theory}, publisher={Institute of Electrical and Electronics Engineers (IEEE)}, author={Tong Zhang}, year={2003}, month={Mar}, pages={682–691}}

@article{Friedman_2001,
	ISSN = {00905364},
	URL = {http://www.jstor.org/stable/2699986},
	abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
	author = {Jerome H. Friedman},
	journal = {The Annals of Statistics},
	number = {5},
	pages = {1189--1232},
	publisher = {Institute of Mathematical Statistics},
	title = {Greedy Function Approximation: A Gradient Boosting Machine},
	volume = {29},
	year = {2001}
}

 @article{Huber_2008, title={On entropy approximation for Gaussian mixture random vectors}, ISBN={9781424421435}, url={http://dx.doi.org/10.1109/MFI.2008.4648062}, DOI={10.1109/mfi.2008.4648062}, journal={2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems}, publisher={IEEE}, author={Huber, Marco F. and Bailey, Tim and Durrant-Whyte, Hugh and Hanebeck, Uwe D.}, year={2008}, month={Aug}}
 
 @article{Osborne_2007,
 	author = {Osborne, Michael and Roberts, Stephen},
 	year = {2007},
 	month = {01},
 	pages = {},
 	title = {Gaussian processes for prediction}
 }

@ARTICLE{Neal_1997,
	author = {{Neal}, Radford M.},
	title = "{Monte Carlo Implementation of Gaussian Process Models for Bayesian Regression and Classification}",
	journal = {arXiv e-prints},
	keywords = {Physics - Data Analysis, Statistics and Probability},
	year = "1997",
	month = "Jan",
	eid = {physics/9701026},
	pages = {physics/9701026},
	archivePrefix = {arXiv},
	eprint = {physics/9701026},
	primaryClass = {physics.data-an},
	adsurl = {https://ui.adsabs.harvard.edu/abs/1997physics...1026N},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

 @article{Petelin_2014, title={Adaptive Importance Sampling for Bayesian Inference in Gaussian Process models}, volume={47}, ISSN={1474-6670}, url={http://dx.doi.org/10.3182/20140824-6-ZA-1003.02352}, DOI={10.3182/20140824-6-za-1003.02352}, number={3}, journal={IFAC Proceedings Volumes}, publisher={Elsevier BV}, author={Petelin, Dejan and Gasperin, Matej and Smídl, Václav}, year={2014}, pages={5011–5016}}
 
 @ARTICLE{Kanagawa_2019,
 	author = {{Kanagawa}, Motonobu and {Hennig}, Philipp},
 	title = "{Convergence Guarantees for Adaptive Bayesian Quadrature Methods}",
 	journal = {arXiv e-prints},
 	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Numerical Analysis, Mathematics - Numerical Analysis, Statistics - Computation},
 	year = "2019",
 	month = "May",
 	eid = {arXiv:1905.10271},
 	pages = {arXiv:1905.10271},
 	archivePrefix = {arXiv},
 	eprint = {1905.10271},
 	primaryClass = {stat.ML},
 	adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190510271K},
 	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
 }

@book{Robert_2001,
	Author = {Robert, Christian P.},
	Publisher = {Springer-Verlag Inc},
	Title = {The Bayesian choice: from decision-theoretic foundations to computational implementation},
	Address = {Berlin; New York},
	Year = {2001}
}

@MISC{Petersen_2012,
	author       = "K. B. Petersen and M. S. Pedersen",
	title        = "The Matrix Cookbook",
	year         = "2012",
	month        = "nov",
	keywords     = "Matrix identity, matrix relations, inverse, matrix derivative",
	publisher    = "Technical University of Denmark",
	address      = "",
	note         = "Version 20121115",
	url          = "http://localhost/pubdb/p.php?3274",
	abstract     = "Matrix identities, relations and approximations. A desktop reference for quick overview of mathematics of matrices."
}

@Article{Booker_1999,
	author="Booker, A. J.
	and Dennis, J. E.
	and Frank, P. D.
	and Serafini, D. B.
	and Torczon, V.
	and Trosset, M. W.",
	title="A rigorous framework for optimization of expensive functions by surrogates",
	journal="Structural optimization",
	year="1999",
	month="Feb",
	day="01",
	volume="17",
	number="1",
	pages="1--13",
	abstract="The goal of the research reported here is to develop rigorous optimization algorithms to apply to some engineering design problems for which direct application of traditional optimization approaches is not practical. This paper presents and analyzes a framework for generating a sequence of approximations to the objective function and managing the use of these approximations as surrogates for optimization. The result is to obtain convergence to a minimizer of an expensive objective function subject to simple constraints. The approach is widely applicable because it does not require, or even explicitly approximate, derivatives of the objective. Numerical results are presented for a 31-variable helicopter rotor blade design example and for a standard optimization test example.",
	issn="1615-1488",
	doi="10.1007/BF01197708",
	url="https://doi.org/10.1007/BF01197708"
}

@Article{Jones_2001,
	author="Jones, Donald R.",
	title="A Taxonomy of Global Optimization Methods Based on Response Surfaces",
	journal="Journal of Global Optimization",
	year="2001",
	month="Dec",
	day="01",
	volume="21",
	number="4",
	pages="345--383",
	abstract="This paper presents a taxonomy of existing approaches for using response surfaces for global optimization. Each method is illustrated with a simple numerical example that brings out its advantages and disadvantages. The central theme is that methods that seem quite reasonable often have non-obvious failure modes. Understanding these failure modes is essential for the development of practical algorithms that fulfill the intuitive promise of the response surface approach.",
	issn="1573-2916",
	doi="10.1023/A:1012771025575",
	url="https://doi.org/10.1023/A:1012771025575"
}

 @article{Asher_2015, title={A review of surrogate models and their application to groundwater modeling}, volume={51}, ISSN={0043-1397}, url={http://dx.doi.org/10.1002/2015WR016967}, DOI={10.1002/2015wr016967}, number={8}, journal={Water Resources Research}, publisher={American Geophysical Union (AGU)}, author={Asher, M. J. and Croke, B. F. W. and Jakeman, A. J. and Peeters, L. J. M.}, year={2015}, month={Aug}, pages={5957–5973}}

 @article{Shahriari_2016, title={Taking the Human Out of the Loop: A Review of Bayesian Optimization}, volume={104}, ISSN={1558-2256}, url={http://dx.doi.org/10.1109/JPROC.2015.2494218}, DOI={10.1109/jproc.2015.2494218}, number={1}, journal={Proceedings of the IEEE}, publisher={Institute of Electrical and Electronics Engineers (IEEE)}, author={Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando}, year={2016}, month={Jan}, pages={148–175}}

@inproceedings{Snoek_2012,
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
	title = {Practical Bayesian Optimization of Machine Learning Algorithms},
	booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
	series = {NIPS'12},
	year = {2012},
	location = {Lake Tahoe, Nevada},
	pages = {2951--2959},
	numpages = {9},
	url = {http://dl.acm.org/citation.cfm?id=2999325.2999464},
	acmid = {2999464},
	publisher = {Curran Associates Inc.},
	address = {USA},
} 

@article{Brochu_2010,
	added-at = {2018-08-13T00:00:00.000+0200},
	author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
	biburl = {https://www.bibsonomy.org/bibtex/212aa0b01d880300d1adc55b333ebf4ae/dblp},
	ee = {http://arxiv.org/abs/1012.2599},
	interhash = {3f3272d1ac9e4d1e667015a512c3a8e1},
	intrahash = {12aa0b01d880300d1adc55b333ebf4ae},
	journal = {CoRR},
	keywords = {dblp},
	timestamp = {2018-08-14T14:34:33.000+0200},
	title = {A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning},
	url = {http://dblp.uni-trier.de/db/journals/corr/corr1012.html#abs-1012-2599},
	volume = {abs/1012.2599},
	year = 2010
}

@article{Hennig_2015,
	author = {Hennig, Philipp and Osborne, Michael A. and Girolami, Mark},
	title = {Probabilistic numerics and uncertainty in computations},
	volume = {471},
	number = {2179},
	year = {2015},
	publisher = {The Royal Society},
	journal = {Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
	link = {http://rspa.royalsocietypublishing.org/content/471/2179/20150142},
	file = {http://rspa.royalsocietypublishing.org/content/royprsa/471/2179/20150142.full.pdf}
}

@article{OHagan_1992,
	title = {Some Bayesian numerical analysis},
	author = {O’Hagan, Anthony},
	journal = {Bayesian Statistics},
	volume = {4},
	number = {345--363},
	pages = {4--2},
	file = {../assets/pdf/OHagan1991.pdf},
	year = {1992}
}

@article{Cockayne_2017,
	author = {{Cockayne}, J. and {Oates}, C. and {Sullivan}, T. and {Girolami}, M.},
	title = {{{B}ayesian Probabilistic Numerical Methods}},
	journal = {ArXiv e-prints},
	volume = {stat.ME 1702.03673},
	year = {2017},
	month = feb,
	link = {https://arxiv.org/abs/1702.03673},
	file = {https://arxiv.org/pdf/1702.03673.pdf}
}

@article{Hoffman_2013,
	author  = {Matthew D. Hoffman and David M. Blei and Chong Wang and John Paisley},
	title   = {Stochastic Variational Inference},
	journal = {Journal of Machine Learning Research},
	year    = {2013},
	volume  = {14},
	pages   = {1303-1347},
	url     = {http://jmlr.org/papers/v14/hoffman13a.html}
}

@inproceedings{Hensman_2012,
	author = {Hensman, James and Rattray, Magnus and Lawrence, Neil D.},
	title = {Fast Variational Inference in the Conjugate Exponential Family},
	booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
	series = {NIPS'12},
	year = {2012},
	location = {Lake Tahoe, Nevada},
	pages = {2888--2896},
	numpages = {9},
	url = {http://dl.acm.org/citation.cfm?id=2999325.2999457},
	acmid = {2999457},
	publisher = {Curran Associates Inc.},
	address = {USA},
}

@article{Smith_1995,
	ISSN = {10618600},
	URL = {http://www.jstor.org/stable/1390762},
	abstract = {One way to estimate variance components is by restricted maximum likelihood. The log-likelihood function is fully defined by the Cholesky factor of a matrix that is usually large and sparse. In this article forward and backward differentiation methods are developed for calculating the first and second derivatives of the Cholesky factor and its functions. These differentiation methods are general and can be applied to either a full or a sparse matrix. Moreover, these methods can be used to calculate the derivatives that are needed for restricted maximum likelihood, resulting in substantial savings in computation.},
	author = {S. P. Smith},
	journal = {Journal of Computational and Graphical Statistics},
	number = {2},
	pages = {134--147},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
	title = {Differentiation of the Cholesky Algorithm},
	volume = {4},
	year = {1995}
}

@ARTICLE{Murray_2016,
	author = {{Murray}, Iain},
	title = "{Differentiation of the Cholesky decomposition}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Computation, Computer Science - Mathematical Software},
	year = "2016",
	month = "Feb",
	eid = {arXiv:1602.07527},
	pages = {arXiv:1602.07527},
	archivePrefix = {arXiv},
	eprint = {1602.07527},
	primaryClass = {stat.CO},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160207527M},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Paszke_2017,
	title={Automatic Differentiation in PyTorch},
	author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	booktitle={NIPS Autodiff Workshop},
	year={2017}
}


@book{Goodfellow_2016,
	title={Deep Learning},
	author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	publisher={MIT Press},
	year={2016}
}

@ARTICLE{Theano_2016,
	author = {{Theano Development Team}},
	title = "{Theano: A {Python} framework for fast computation of mathematical expressions}",
	journal = {arXiv e-prints},
	volume = {abs/1605.02688},
	primaryClass = "cs.SC",
	keywords = {Computer Science - Symbolic Computation, Computer Science - Learning, Computer Science - Mathematical Software},
	year = 2016,
	month = may,
	url = {http://arxiv.org/abs/1605.02688},
}

@misc{Abadi_2015,
	title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url={https://www.tensorflow.org/},
	note={Software available from tensorflow.org},
	author={
	Mart\'{\i}n~Abadi et al.},
	year={2015},
}

@article{Salvatier_2016,
	doi = {10.7717/peerj-cs.55},
	url = {https://doi.org/10.7717/peerj-cs.55},
	year  = {2016},
	month = {apr},
	publisher = {{PeerJ}},
	volume = {2},
	pages = {e55},
	author = {John Salvatier and Thomas V. Wiecki and Christopher Fonnesbeck},
	title = {Probabilistic programming in Python using {PyMC}3},
	journal = {{PeerJ} Computer Science}
}

 @article{Foreman_Mackey_2013, title={emcee: The MCMC Hammer}, volume={125}, ISSN={1538-3873}, url={http://dx.doi.org/10.1086/670067}, DOI={10.1086/670067}, number={925}, journal={Publications of the Astronomical Society of the Pacific}, publisher={IOP Publishing}, author={Foreman-Mackey, Daniel and Hogg, David W. and Lang, Dustin and Goodman, Jonathan}, year={2013}, month={Mar}, pages={306–312}}

@ARTICLE{Andrieu_2009,
	author = {Christophe Andrieu and Gareth O. Roberts},
	title = {The pseudo-marginal approach for efficient Monte Carlo computations},
	journal = {Annals of Statistics},
	year = {2009},
	pages = {697--725}
}

 @article{Andrieu_2010, title={Particle Markov chain Monte Carlo methods}, volume={72}, ISSN={1467-9868}, url={http://dx.doi.org/10.1111/j.1467-9868.2009.00736.x}, DOI={10.1111/j.1467-9868.2009.00736.x}, number={3}, journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)}, publisher={Wiley}, author={Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman}, year={2010}, month={Jun}, pages={269–342}}

 @article{Drovandi_2018, title={Accelerating pseudo-marginal MCMC using Gaussian processes}, volume={118}, ISSN={0167-9473}, url={http://dx.doi.org/10.1016/j.csda.2017.09.002}, DOI={10.1016/j.csda.2017.09.002}, journal={Computational Statistics & Data Analysis}, publisher={Elsevier BV}, author={Drovandi, Christopher C. and Moores, Matthew T. and Boys, Richard J.}, year={2018}, month={Feb}, pages={1–17}}

@article{Sherlock_2015,
	author = "Sherlock, Chris and Thiery, Alexandre H. and Roberts, Gareth O. and Rosenthal, Jeffrey S.",
	doi = "10.1214/14-AOS1278",
	fjournal = "The Annals of Statistics",
	journal = "Ann. Statist.",
	month = "02",
	number = "1",
	pages = "238--275",
	publisher = "The Institute of Mathematical Statistics",
	title = "On the efficiency of pseudo-marginal random walk Metropolis algorithms",
	url = "https://doi.org/10.1214/14-AOS1278",
	volume = "43",
	year = "2015"
}

 @article{Pritchard_1999, title={Population growth of human Y chromosomes: a study of Y chromosome microsatellites}, volume={16}, ISSN={1537-1719}, url={http://dx.doi.org/10.1093/oxfordjournals.molbev.a026091}, DOI={10.1093/oxfordjournals.molbev.a026091}, number={12}, journal={Molecular Biology and Evolution}, publisher={Oxford University Press (OUP)}, author={Pritchard, J. K. and Seielstad, M. T. and Perez-Lezaun, A. and Feldman, M. W.}, year={1999}, month={Dec}, pages={1791–1798}}

 @article{Fearnhead_2012, title={Constructing summary statistics for approximate Bayesian computation: semi-automatic approximate Bayesian computation}, volume={74}, ISSN={1369-7412}, url={http://dx.doi.org/10.1111/j.1467-9868.2011.01010.x}, DOI={10.1111/j.1467-9868.2011.01010.x}, number={3}, journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)}, publisher={Wiley}, author={Fearnhead, Paul and Prangle, Dennis}, year={2012}, month={May}, pages={419–474}}

@article{Beaumont_2003,
	author = {A Beaumont, Mark and Zhang, Wenyang and J Balding, David},
	year = {2003},
	month = {01},
	pages = {2025-35},
	title = {Approximate Bayesian Computation in Population Genetics},
	volume = {162},
	journal = {Genetics}
}

 @article{Marjoram_2003, title={Markov chain Monte Carlo without likelihoods}, volume={100}, ISSN={1091-6490}, url={http://dx.doi.org/10.1073/pnas.0306899100}, DOI={10.1073/pnas.0306899100}, number={26}, journal={Proceedings of the National Academy of Sciences}, publisher={Proceedings of the National Academy of Sciences}, author={Marjoram, P. and Molitor, J. and Plagnol, V. and Tavare, S.}, year={2003}, month={Dec}, pages={15324–15328}}
 
 @book{Tarantola_2004,
 	author = {Tarantola, Albert},
 	title = {Inverse Problem Theory and Methods for Model Parameter Estimation},
 	year = {2004},
 	isbn = {0898715725},
 	publisher = {Society for Industrial and Applied Mathematics},
 	address = {Philadelphia, PA, USA},
 } 

 @article{Marzouk_2007, title={Stochastic spectral methods for efficient Bayesian solution of inverse problems}, volume={224}, ISSN={0021-9991}, url={http://dx.doi.org/10.1016/j.jcp.2006.10.010}, DOI={10.1016/j.jcp.2006.10.010}, number={2}, journal={Journal of Computational Physics}, publisher={Elsevier BV}, author={Marzouk, Youssef M. and Najm, Habib N. and Rahn, Larry A.}, year={2007}, month={Jun}, pages={560–586}}

@article{Conrad_2016,
	author = {Patrick R. Conrad and Youssef M. Marzouk and Natesh S. Pillai and Aaron Smith},
	title = {Accelerating Asymptotically Exact MCMC for Computationally Intensive Models via Local Approximations},
	journal = {Journal of the American Statistical Association},
	volume = {111},
	number = {516},
	pages = {1591-1607},
	year  = {2016},
	publisher = {Taylor & Francis},
	doi = {10.1080/01621459.2015.1096787},
	
	URL = { 
	https://doi.org/10.1080/01621459.2015.1096787
	
	},
	eprint = { 
	https://doi.org/10.1080/01621459.2015.1096787
	
	}
	
}

@article{Bilionis_2013,
	doi = {10.1088/0266-5611/30/1/015004},
	url = {https://doi.org/10.1088%2F0266-5611%2F30%2F1%2F015004},
	year = 2013,
	month = {dec},
	publisher = {{IOP} Publishing},
	volume = {30},
	number = {1},
	pages = {015004},
	author = {I Bilionis and N Zabaras},
	title = {Solution of inverse problems with limited forward solver evaluations: a Bayesian perspective},
	journal = {Inverse Problems},
}

 @article{Wang_2018_2, title={Adaptive Gaussian Process Approximation for Bayesian Inference with Expensive Likelihood Functions}, volume={30}, ISSN={1530-888X}, url={http://dx.doi.org/10.1162/neco_a_01127}, DOI={10.1162/neco_a_01127}, number={11}, journal={Neural Computation}, publisher={MIT Press - Journals}, author={Wang, Hongqiao and Li, Jinglai}, year={2018}, month={Nov}, pages={3072–3094}}

@inproceedings{Rasmussen_2003,
	title = {Gaussian Processes to Speed up Hybrid Monte Carlo for Expensive Bayesian Integrals},
	author = {Rasmussen, CE.},
	journal = {Bayesian Statistics 7},
	pages = {651-659},
	organization = {Max-Planck-Gesellschaft},
	school = {Biologische Kybernetik},
	year = {2003}
}

 @article{Bliznyuk_2012, title={Local Derivative-Free Approximation of Computationally Expensive Posterior Densities}, volume={21}, ISSN={1537-2715}, url={http://dx.doi.org/10.1080/10618600.2012.681255}, DOI={10.1080/10618600.2012.681255}, number={2}, journal={Journal of Computational and Graphical Statistics}, publisher={Informa UK Limited}, author={Bliznyuk, Nikolay and Ruppert, David and Shoemaker, Christine A.}, year={2012}, month={Apr}, pages={476–495}}

@inproceedings{Kandasamy_2015,
	author = {Kandasamy, Kirthevasan and Schneider, Jeff and Póczos, Barnabás},
	title = {Bayesian Active Learning for Posterior Estimation},
	booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
	series = {IJCAI'15},
	year = {2015},
	isbn = {978-1-57735-738-4},
	location = {Buenos Aires, Argentina},
	pages = {3605--3611},
	numpages = {7},
	url = {http://dl.acm.org/citation.cfm?id=2832747.2832752},
	acmid = {2832752},
	publisher = {AAAI Press},
} 

 @article{Ghahramani_2015, title={Probabilistic machine learning and artificial intelligence}, volume={521}, ISSN={1476-4687}, url={http://dx.doi.org/10.1038/nature14541}, DOI={10.1038/nature14541}, number={7553}, journal={Nature}, publisher={Springer Science and Business Media LLC}, author={Ghahramani, Zoubin}, year={2015}, month={May}, pages={452–459}}
 
 @InProceedings{Mohammed_2017,
 	author="Mohammed, Rekar O.
 	and Cawley, Gavin C.",
 	title="Over-Fitting in Model Selection with Gaussian Process Regression",
 	booktitle="Machine Learning and Data Mining in Pattern Recognition",
 	year="2017",
 	publisher="Springer International Publishing",
 	address="Cham",
 	pages="192--205",
 	abstract="Model selection in Gaussian Process Regression (GPR) seeks to determine the optimal values of the hyper-parameters governing the covariance function, which allows flexible customization of the GP to the problem at hand. An oft-overlooked issue that is often encountered in the model process is over-fitting the model selection criterion, typically the marginal likelihood. The over-fitting in machine learning refers to the fitting of random noise present in the model selection criterion in addition to features improving the generalisation performance of the statistical model. In this paper, we construct several Gaussian process regression models for a range of high-dimensional datasets from the UCI machine learning repository. Afterwards, we compare both MSE on the test dataset and the negative log marginal likelihood (nlZ), used as the model selection criteria, to find whether the problem of overfitting in model selection also affects GPR. We found that the squared exponential covariance function with Automatic Relevance Determination (SEard) is better than other kernels including squared exponential covariance function with isotropic distance measure (SEiso) according to the nLZ, but it is clearly not the best according to MSE on the test data, and this is an indication of over-fitting problem in model selection.",
 	isbn="978-3-319-62416-7"
 }
 
 @article{Mermin_2004, title={Could Feynman Have Said This?}, volume={57}, ISSN={1945-0699}, url={http://dx.doi.org/10.1063/1.1768652}, DOI={10.1063/1.1768652}, number={5}, journal={Physics Today}, publisher={AIP Publishing}, author={Mermin, N. David}, year={2004}, month={May}, pages={10–11}}

@incollection{Geyer_2011,
	author      = "C. J. Geyer",
	title       = " Introduction to Markov Chain Monte Carlo",
	editor      = " A. Gelman, S. Brooks, G. Jones and X. L. Meng",
	booktitle   = " Handbook of Markov Chain Monte Carlo: Methods and Applications",
	publisher   = "CRC Press",
	address     = "London",
	year        = 2004
}

@article{Bassett_2018,
	title={Maximum a posteriori estimators as a limit of Bayes estimators},
	volume={174},
	ISSN={1436-4646},
	url={http://dx.doi.org/10.1007/s10107-018-1241-0},
	DOI={10.1007/s10107-018-1241-0},
	number={1-2},
	journal={Mathematical Programming},
	publisher={Springer Nature},
	author={Bassett, Robert and Deride, Julio},
	year={2018},
	month={Jan},
	pages={129–144}
}

 @article{Stuart_2010, title={Inverse problems: A Bayesian perspective}, volume={19}, ISSN={1474-0508}, url={http://dx.doi.org/10.1017/S0962492910000061}, DOI={10.1017/s0962492910000061}, journal={Acta Numerica}, publisher={Cambridge University Press (CUP)}, author={Stuart, A. M.}, year={2010}, month={May}, pages={451–559}}
